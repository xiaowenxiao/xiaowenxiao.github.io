	<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.79.0" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <title> K8S-使用Ceph存储 &middot; x&#39;Blog </title>
  <link rel="stylesheet" href="https://xiaowenxiao.github.io/css/poole.css">
  <link rel="stylesheet" href="https://xiaowenxiao.github.io/css/syntax.css">
  <link rel="stylesheet" href="https://xiaowenxiao.github.io/css/we.css">
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">
  

  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?xxxxxxxxxxxxxxxxxxxxxxxxxxxxx";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>

</head>
	<body>
	<div class="container">
	<header>
    <h3 class="site-logo"><a href="https://xiaowenxiao.github.io/">x&#39;Blog</a></h3>
    <nav><ul class="site-menu"><li><a href="/about">关于我</a></li><li><a href="/categories/ceph">ceph</a></li><li><a href="/categories/k8s">k8s</a></li><li><a href="/categories/linux">linux</a></li><li><a href="https://xiaowenxiao.github.io/index.xml">RSS</a></li></ul></nav>
</header>


		<div class="content">
			<div class="post">
			      <p>[toc]</p>
<h1 id="pvpvc概述">PV、PVC概述</h1>
<p>管理存储是管理计算的一个明显问题。PersistentVolume子系统为用户和管理员提供了一个API，用于抽象如何根据消费方式提供存储的详细信息。于是引入了两个新的API资源：PersistentVolume和PersistentVolumeClaim</p>
<blockquote>
<p>PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象包含存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。</p>
</blockquote>
<blockquote>
<p>PersistentVolumeClaim（PVC）是用户存储的请求。 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式。</p>
</blockquote>
<blockquote>
<p>虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的是，用户需要具有不同属性（如性能）的PersistentVolumes，用于不同的问题。 管理员需要能够提供多种不同于PersistentVolumes，而不仅仅是大小和访问模式，而不会使用户了解这些卷的实现细节。 对于这些需求，存在StorageClass资源。</p>
</blockquote>
<blockquote>
<p>StorageClass为集群提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为“配置文件”</p>
</blockquote>
<h1 id="pod动态供给">POD动态供给</h1>
<blockquote>
<p>动态供给主要是能够自动帮你创建pv，需要多大的空间就创建多大的pv。k8s帮助创建pv，创建pvc就直接api调用存储类来寻找pv。</p>
</blockquote>
<blockquote>
<p>如果是存储静态供给的话，会需要我们手动去创建pv，如果没有足够的资源，找不到合适的pv，那么pod就会处于pending等待的状态。而动态供给主要的一个实现就是StorageClass存储对象，其实它就是声明你使用哪个存储，然后帮你去连接，再帮你去自动创建pv。</p>
</blockquote>
<h1 id="pod使用rbd做为持久数据卷">POD使用RBD做为持久数据卷</h1>
<h3 id="安装与配置">安装与配置</h3>
<p>RBD支持ReadWriteOnce，ReadOnlyMany两种模式<br>
1、配置rbd-provisioner</p>
<pre><code>cat &gt;external-storage-rbd-provisioner.yaml&lt;&lt;EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rbd-provisioner
  namespace: kube-system
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumeclaims&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]
  - apiGroups: [&quot;storage.k8s.io&quot;]
    resources: [&quot;storageclasses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;events&quot;]
    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;endpoints&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;services&quot;]
    resourceNames: [&quot;kube-dns&quot;]
    verbs: [&quot;list&quot;, &quot;get&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
subjects:
  - kind: ServiceAccount
    name: rbd-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: rbd-provisioner
  apiGroup: rbac.authorization.k8s.io

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rbd-provisioner
  namespace: kube-system
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  verbs: [&quot;get&quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rbd-provisioner
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rbd-provisioner
subjects:
- kind: ServiceAccount
  name: rbd-provisioner
  namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rbd-provisioner
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: rbd-provisioner
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: rbd-provisioner
    spec:
      containers:
      - name: rbd-provisioner
        image: &quot;quay.io/external_storage/rbd-provisioner:lastest&quot;
        env:
        - name: PROVISIONER_NAME
          value: ceph.com/rbd
      serviceAccount: rbd-provisioner
EOF
kubectl apply -f external-storage-rbd-provisioner.yaml
# 查看状态 等待running之后 再进行后续的操作
kubectl get pod -n kube-system
</code></pre><p>2、配置storageclass</p>
<pre><code>1、创建pod时，kubelet需要使用rbd命令去检测和挂载pv对应的ceph image，所以要在所有的worker节点安装ceph客户端ceph-common。
将ceph的ceph.client.admin.keyring和ceph.conf文件拷贝到master的/etc/ceph目录下
yum -y install ceph-common
2、创建 osd pool 在ceph的mon或者admin节点
ceph osd pool create kube 128 128 
ceph osd pool ls
3、创建k8s访问ceph的用户 在ceph的mon或者admin节点
ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring
4、查看key 在ceph的mon或者admin节点
ceph auth get-key client.admin
ceph auth get-key client.kube
5、创建 admin secret
kubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; \
--from-literal=key=AQCtovZdgFEhARAAoKhLtquAyM8ROvmBv55Jig== \
--namespace=kube-system
6、在 default 命名空间创建pvc用于访问ceph的 secret
kubectl create secret generic ceph-user-secret --type=&quot;kubernetes.io/rbd&quot; \
--from-literal=key=AQAM9PxdEFi3AhAAzvvhuyk1AfN5twlY+4zNMA== \
--namespace=default

</code></pre><p>3、配置StorageClass</p>
<pre><code>cat &gt;storageclass-ceph-rdb.yaml&lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: dynamic-ceph-rdb
provisioner: ceph.com/rbd
parameters:
  monitors: 192.168.0.201:6789,192.168.0.202:6789,192.168.0.203:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: kube-system
  pool: kube
  userId: kube
  userSecretName: ceph-user-secret
  fsType: ext4
  imageFormat: &quot;2&quot;
  imageFeatures: &quot;layering&quot;
EOF
</code></pre><p>4、创建yaml</p>
<pre><code>kubectl apply -f storageclass-ceph-rdb.yaml
</code></pre><p>5、查看sc</p>
<pre><code>kubectl get storageclasses 
</code></pre><h3 id="测试使用">测试使用</h3>
<p>1、创建pvc测试</p>
<pre><code>cat &gt;ceph-rdb-pvc-test.yaml&lt;&lt;EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-rdb-claim
spec:
  accessModes:     
    - ReadWriteOnce
  storageClassName: dynamic-ceph-rdb
  resources:
    requests:
      storage: 2Gi
EOF
kubectl apply -f ceph-rdb-pvc-test.yaml
</code></pre><p>2、查看</p>
<pre><code>kubectl get pvc
kubectl get pv
</code></pre><p>3、创建 nginx pod 挂载测试</p>
<pre><code>cat &gt;nginx-pod.yaml&lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod1
  labels:
    name: nginx-pod1
spec:
  containers:
  - name: nginx-pod1
    image: nginx:alpine
    ports:
    - name: web
      containerPort: 80
    volumeMounts:
    - name: ceph-rdb
      mountPath: /usr/share/nginx/html
  volumes:
  - name: ceph-rdb
    persistentVolumeClaim:
      claimName: ceph-rdb-claim
EOF
kubectl apply -f nginx-pod.yaml
</code></pre><p>4、查看</p>
<pre><code>kubectl get pods -o wide
</code></pre><p>5、修改文件内容</p>
<pre><code>kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo this is from Ceph RBD!!! &gt; /usr/share/nginx/html/index.html'
</code></pre><p>6、访问测试</p>
<pre><code>curl http://$podip
</code></pre><p>7、清理</p>
<pre><code>kubectl delete -f nginx-pod.yaml
kubectl delete -f ceph-rdb-pvc-test.yaml
</code></pre><h1 id="pod使用cephfs做为持久数据卷">POD使用CephFS做为持久数据卷</h1>
<p>CephFS方式支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany</p>
<h2 id="ceph端创建cephfs-pool">Ceph端创建CephFS pool</h2>
<p>1、如下操作在ceph的mon或者admin节点
CephFS需要使用两个Pool来分别存储数据和元数据</p>
<pre><code>ceph osd pool create fs_data 128
ceph osd pool create fs_metadata 128
ceph osd lspools
</code></pre><p>2、创建一个CephFS</p>
<pre><code>ceph fs new cephfs fs_metadata fs_data
</code></pre><p>3、查看</p>
<pre><code>ceph fs ls
</code></pre><h2 id="部署-cephfs-provisioner">部署 cephfs-provisioner</h2>
<p>1、使用社区提供的cephfs-provisioner</p>
<pre><code>cat &gt;external-storage-cephfs-provisioner.yaml&lt;&lt;EOF
apiVersion: v1
kind: Namespace
metadata:
   name: cephfs
   labels:
     name: cephfs
 
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cephfs-provisioner
  namespace: kube-system
 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cephfs-provisioner
  namespace: kube-system
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;secrets&quot;]
    verbs: [&quot;create&quot;, &quot;get&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;endpoints&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
 
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cephfs-provisioner
  namespace: kube-system
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumeclaims&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]
  - apiGroups: [&quot;storage.k8s.io&quot;]
    resources: [&quot;storageclasses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;events&quot;]
    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;services&quot;]
    resourceNames: [&quot;kube-dns&quot;,&quot;coredns&quot;]
    verbs: [&quot;list&quot;, &quot;get&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;secrets&quot;]
    verbs: [&quot;get&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;policy&quot;]
    resourceNames: [&quot;cephfs-provisioner&quot;]
    resources: [&quot;podsecuritypolicies&quot;]
    verbs: [&quot;use&quot;]
 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cephfs-provisioner
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cephfs-provisioner
subjects:
- kind: ServiceAccount
  name: cephfs-provisioner
 
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cephfs-provisioner
subjects:
  - kind: ServiceAccount
    name: cephfs-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cephfs-provisioner
  apiGroup: rbac.authorization.k8s.io
 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cephfs-provisioner
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: cephfs-provisioner
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: cephfs-provisioner
    spec:
      containers:
      - name: cephfs-provisioner
        image: &quot;quay.io/external_storage/cephfs-provisioner:latest&quot;
        env:
        - name: PROVISIONER_NAME
          value: ceph.com/cephfs
        command:
        - &quot;/usr/local/bin/cephfs-provisioner&quot;
        args:
        - &quot;-id=cephfs-provisioner-1&quot;
        - &quot;-disable-ceph-namespace-isolation=true&quot;
      serviceAccount: cephfs-provisioner
EOF
kubectl apply -f external-storage-cephfs-provisioner.yaml
</code></pre><p>2、查看状态 等待running之后 再进行后续的操作</p>
<pre><code>kubectl get pod -n kube-system
</code></pre><h2 id="配置-storageclass">配置 storageclass</h2>
<p>1、查看key 在ceph的mon或者admin节点</p>
<pre><code>ceph auth get-key client.admin
</code></pre><p>2、创建 admin secret</p>
<pre><code>kubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; \
--from-literal=key=AQCtovZdgFEhARAAoKhLtquAyM8ROvmBv55Jig== \
--namespace=kube-system

</code></pre><p>3、查看 secret</p>
<pre><code>kubectl get secret ceph-secret -n kube-system -o yaml
</code></pre><p>4、配置 StorageClass</p>
<pre><code>cat &gt;storageclass-cephfs.yaml&lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: dynamic-cephfs
provisioner: ceph.com/cephfs
parameters:
    monitors: 192.168.0.201:6789,192.168.0.202:6789,192.168.0.203:6789
    adminId: admin
    adminSecretName: ceph-secret
    adminSecretNamespace: &quot;kube-system&quot;
    claimRoot: /volumes/kubernetes
EOF
</code></pre><p>5、创建</p>
<pre><code>kubectl apply -f storageclass-cephfs.yaml
</code></pre><p>6、查看</p>
<pre><code>kubectl get sc
</code></pre><h2 id="测试使用-1">测试使用</h2>
<p>1、创建pvc测试</p>
<pre><code>cat &gt;cephfs-pvc-test.yaml&lt;&lt;EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephfs-claim
spec:
  accessModes:     
    - ReadWriteOnce
  storageClassName: dynamic-cephfs
  resources:
    requests:
      storage: 2Gi
EOF
kubectl apply -f cephfs-pvc-test.yaml
</code></pre><p>2、查看</p>
<pre><code>kubectl get pvc
kubectl get pv
</code></pre><p>3、创建 nginx pod 挂载测试</p>
<pre><code>cat &gt;nginx-pod.yaml&lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod2
  labels:
    name: nginx-pod2
spec:
  containers:
  - name: nginx-pod2
    image: nginx
    ports:
    - name: web
      containerPort: 80
    volumeMounts:
    - name: cephfs
      mountPath: /usr/share/nginx/html
  volumes:
  - name: cephfs
    persistentVolumeClaim:
      claimName: cephfs-claim
EOF
kubectl apply -f nginx-pod.yaml
</code></pre><p>4、查看</p>
<pre><code>kubectl get pods -o wide
</code></pre><p>5、修改文件内容</p>
<pre><code>kubectl exec -ti nginx-pod2 -- /bin/sh -c 'echo This is from CephFS!!! &gt; /usr/share/nginx/html/index.html'
</code></pre><p>6、访问pod测试</p>
<pre><code>curl http://$podip
</code></pre><p>7、清理</p>
<pre><code>kubectl delete -f nginx-pod.yaml
kubectl delete -f cephfs-pvc-test.yaml
</code></pre>
			</div>
			
            

		</div>
</div>
	</body>
</html>
